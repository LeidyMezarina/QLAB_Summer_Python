{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with LLMs\n",
    "\n",
    "Modern LLMs have been trained on very large datasets to be able to predict appropiate responses to queries. Through this training, they also become able to predict classifications of queries, and they can be instructed to return these classifications by specifying their task through the system prompt.\n",
    "\n",
    "For this implementation, the system pompt is very important, as we can give information about the different classifications, and even feed the model examples of correct classifications. When we don't feed these examples, the process is called \"zero-shot\" classification, as we haven't provided examples of the desired behavior. Otherwise, the process is called \"few-shot\", in which case the system prompt contains examples of the desired response.\n",
    "\n",
    "We will now build a few-shot prompt to instruct the model to classify whether a query is a question about biology or a question about finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from Constants import OPENAI_API_KEY\n",
    "\n",
    "delimiter = \"####\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You will be provided with user queries and your task is to classify whether they are about finance or about biology. \n",
    "\n",
    "The user queries will be delimited with {delimiter} characters\n",
    "\n",
    "As an output, provide one word, either \"Finance\" or \"Biology\"\n",
    "\n",
    "Some examples of queries and how you should respond to them:\n",
    "\n",
    "{delimiter}What is a credit default swap?{delimiter}\n",
    "Finance\n",
    "\n",
    "{delimiter}What are the mitochondria?{delimiter}\n",
    "Biology\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot prompts raise the effectiveness of the model at generating the desired responses. Generally, the system prompt is one of the most important parts of an application, as it largely determines how the model will generate text for upcoming queries. The process of designing a prompt to specify the desired behavior is called \"prompt engineering\". When building an app that relies on LLMs, testing and comparing the responses of the model after using different prompts is a very useful task that leads to better prompt design\n",
    "\n",
    "Now we use the API to finish the classification task. The steps are the same as in last Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finance'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the difference between a Roth IRA and a 401(k)?\"\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{delimiter}{query}{delimiter}\"}\n",
    "]\n",
    "response = client.chat.completions.create(model = model, messages = messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Biology'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the life cycle of fungi?\"\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{delimiter}{query}{delimiter}\"}\n",
    "]\n",
    "response = client.chat.completions.create(model = model, messages = messages)\n",
    "response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
